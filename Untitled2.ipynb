{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,csv,random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing import text\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.layers import Input,Dense,merge, LSTM\n",
    "from get_smiley import happy , sad\n",
    "from linguistic_features import feature_getter,pos_getter,get_wotscore , num_stop_words ,num_punc\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sl=defaultdict(int)\n",
    "for i in open(\"list_swear\",'r').readlines():\n",
    "\ti=i.strip().lower()\n",
    "\tsl[i]=1\n",
    "\n",
    "pos_e=defaultdict(int)\n",
    "for i in open(\"postive_emotion\",'r').readlines():\n",
    "\ti=i.strip().lower()\n",
    "\tpos_e[i]=1\n",
    "\n",
    "neg_e=defaultdict(int)\n",
    "for i in open(\"negative_emotion\",'r').readlines():\n",
    "\ti=i.strip().lower()\n",
    "\tneg_e[i]=1\n",
    "\n",
    "def pos_emo(text):\n",
    "\twords=text.split(\" \")\n",
    "\tsc=0\n",
    "\tfor word in words:\n",
    "\t\tif word.lower() in pos_e:\n",
    "\t\t\tsc+=1\n",
    "\treturn sc\n",
    "\n",
    "def neg_emo(text):\n",
    "\twords=text.split(\" \")\n",
    "\tsc=0\n",
    "\tfor word in words:\n",
    "\t\tif word.lower() in neg_e:\n",
    "\t\t\tsc+=1\n",
    "\treturn sc\n",
    "\n",
    "\n",
    "def swear_number(text):\n",
    "\twords=text.split(\" \")\n",
    "\tsc=0\n",
    "\tfor word in words:\n",
    "\t\tif word.lower() in sl:\n",
    "\t\t\tsc=+1\n",
    "\treturn sc\n",
    "\n",
    "\n",
    "def get_features(tweet):\n",
    "\tfeatures=[]\n",
    "\tfeatures.append(len(str(tweet)))\n",
    "\tfeatures.append(len(str(tweet).split(\" \")))\n",
    "\tfeatures.append(len(set(str(tweet))))\n",
    "\tfeatures.append(happy(str(tweet)))\n",
    "\tfeatures.append(sad(str(tweet)))\n",
    "\tfeatures.append(swear_number(str(tweet)))\n",
    "\tfeatures.append(pos_emo(str(tweet)))\n",
    "\tfeatures.append(neg_emo(str(tweet)))\n",
    "\tfeatures=features+feature_getter(tweet)\n",
    "\tfeatures=features+pos_getter(str(tweet))\n",
    "\tfeatures.append(num_stop_words(str(tweet)))\n",
    "\tfeatures.append(num_punc(str(tweet)))\n",
    "\treturn features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp=open('training.1600000.processed.noemoticon.csv','r')\n",
    "reader=csv.reader(fp)\n",
    "data1=[]\n",
    "for line in reader:\n",
    "\ttemp_data=line[len(line)-1]\n",
    "\tlabel=line[0]\n",
    "\twords=temp_data.split()\n",
    "\tfor i in range(0,len(words)):\n",
    "\t\tif(\"http\" in words[i]):\n",
    "\t\t\twords[i]=''\n",
    "\t\telse:\n",
    "\t\t\twords[i]=words[i].lower()\n",
    "\ttemp_data=' '.join(words)\n",
    "\tdata1.append((temp_data,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('read complete')\n",
    "random.shuffle(data1)\n",
    "data=data1[:4000]\n",
    "print('shuffle complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L=len(data)\n",
    "train_data=[i[0] for i in data[:int(0.8*L)]]\n",
    "train_labels=[i[1] for i in data[:int(0.8*L)]]\n",
    "train_features=[]\n",
    "for i in train_data:\n",
    "    i=re.sub(r'[^\\x00-\\x7F]+',' ', i)\n",
    "    train_features.append(np.asarray(get_features(i)))\n",
    "test_data=[i[0] for i in data[int(0.8*L):]]\n",
    "test_labels=[i[1] for i in data[int(0.8*L):]]\n",
    "test_features=[]\n",
    "for i in test_data:\n",
    "        test_features.append(np.asarray(get_features(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features=np.asarray(train_features)\n",
    "test_features=np.asarray(test_features)\n",
    "\n",
    "X = np.concatenate((train_data, test_data), axis=0)\n",
    "y = np.concatenate((train_labels, test_labels), axis=0)\n",
    "\n",
    "vocabulory=np.unique(np.hstack(X))\n",
    "X_train=[]\n",
    "X_test=[]\n",
    "for i in train_data:\n",
    "\tX_train.append(text.one_hot(i,len(vocabulory)))\n",
    "for i in test_data:\n",
    "\tX_test.append(text.one_hot(i,len(vocabulory)))\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=300)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=300)\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(train_labels)\n",
    "encoded_Y=encoder.transform(train_labels)\n",
    "y_train=np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(test_labels)\n",
    "encoded_Y=encoder.transform(test_labels)\n",
    "y_test=np_utils.to_categorical(encoded_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff95338fc50>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputt=Input(shape=(300,))\n",
    "embedding=Embedding(len(vocabulory), 32, input_length=300)(inputt)\n",
    "conv=Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(embedding)\n",
    "pol=MaxPooling1D(pool_size=2)(conv)\n",
    "den=Dense(300, activation='relu')(pol)\n",
    "seq_features=Flatten()(den)\n",
    "nb_features=len(train_features[0])\n",
    "other_features=Input(shape=(nb_features,))\n",
    "\n",
    "model_final=  merge([seq_features , other_features], mode='concat')\n",
    "#model_final = Dense(250, activation='relu')(model_final)\n",
    "model_final = Dense(2, activation='softmax')(model_final)\n",
    "model_final = Model([inputt, other_features], model_final)\n",
    "\n",
    "model_final.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_final.fit([X_train,train_features], y_train,validation_data=([X_test,test_features],y_test), epochs=2, batch_size=128, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model expects 2 input arrays, but only received one array. Found: array with shape (3200, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-34282eb5f8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1379\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    109\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                              \u001b[0;34m' arrays, but only received one array. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                              'Found: array with shape ' + str(data.shape))\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model expects 2 input arrays, but only received one array. Found: array with shape (3200, 300)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulory), 32, input_length=300))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_final.fit(X_train, y_train,validation_data=(X_test,y_test), epochs=2, batch_size=128, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
